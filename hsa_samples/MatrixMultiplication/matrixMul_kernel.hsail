//==============================================================================
// Copyright (c) 2015 Advanced Micro Devices, Inc. All rights reserved.
//
/// \author AMD Developer Tools
/// \file
//==============================================================================
module &m:1:0:$full:$large:$default;
extension "amd:gcn";
extension "IMAGE";

decl prog function &abort()();

prog kernel &__OpenCL_matrixMul_kernel(
    kernarg_u64 %__global_offset_0,
    kernarg_u64 %__global_offset_1,
    kernarg_u64 %__global_offset_2,
    kernarg_u64 %__printf_buffer,
    kernarg_u64 %__vqueue_pointer,
    kernarg_u64 %__aqlwrap_pointer,
    kernarg_u64 %C,
    kernarg_u64 %A,
    kernarg_u64 %B,
    kernarg_u32 %wA,
    kernarg_u32 %wB)
{
    pragma  "AMD RTI", "ARGSTART:__OpenCL_matrixMul_kernel";
    pragma  "AMD RTI", "version:3:1:104";
    pragma  "AMD RTI", "device:generic";
    pragma  "AMD RTI", "uniqueid:1024";
    pragma  "AMD RTI", "memory:private:0";
    pragma  "AMD RTI", "memory:region:0";
    pragma  "AMD RTI", "memory:local:0";
    pragma  "AMD RTI", "value:__global_offset_0:u64:1:1:0";
    pragma  "AMD RTI", "value:__global_offset_1:u64:1:1:16";
    pragma  "AMD RTI", "value:__global_offset_2:u64:1:1:32";
    pragma  "AMD RTI", "pointer:__printf_buffer:u8:1:1:48:uav:7:1:RW:0:0:0";
    pragma  "AMD RTI", "value:__vqueue_pointer:u64:1:1:64";
    pragma  "AMD RTI", "value:__aqlwrap_pointer:u64:1:1:80";
    pragma  "AMD RTI", "pointer:C:float:1:1:96:uav:7:4:RW:0:0:0";
    pragma  "AMD RTI", "pointer:A:float:1:1:112:uav:7:4:RW:0:0:0";
    pragma  "AMD RTI", "pointer:B:float:1:1:128:uav:7:4:RW:0:0:0";
    pragma  "AMD RTI", "value:wA:u32:1:1:144";
    pragma  "AMD RTI", "value:wB:u32:1:1:160";
    pragma  "AMD RTI", "function:1:0";
    pragma  "AMD RTI", "memory:64bitABI";
    pragma  "AMD RTI", "privateid:8";
    pragma  "AMD RTI", "enqueue_kernel:0";
    pragma  "AMD RTI", "kernel_index:0";
    pragma  "AMD RTI", "reflection:0:size_t";
    pragma  "AMD RTI", "reflection:1:size_t";
    pragma  "AMD RTI", "reflection:2:size_t";
    pragma  "AMD RTI", "reflection:3:size_t";
    pragma  "AMD RTI", "reflection:4:size_t";
    pragma  "AMD RTI", "reflection:5:size_t";
    pragma  "AMD RTI", "reflection:6:float*";
    pragma  "AMD RTI", "reflection:7:float*";
    pragma  "AMD RTI", "reflection:8:float*";
    pragma  "AMD RTI", "reflection:9:int";
    pragma  "AMD RTI", "reflection:10:int";
    pragma  "AMD RTI", "ARGEND:__OpenCL_matrixMul_kernel";

@__OpenCL_matrixMul_kernel_entry:
    // BB#0:                                // %entry
    workitemabsid_u32   $s0, 0;
    cvt_u64_u32 $d5, $s0;           // $d5 = work_item_id_x (tx)
    workitemabsid_u32   $s0, 1;
    ld_kernarg_align(8)_width(all)_u64  $d6, [%__global_offset_0]; // $d6 = 0
    add_u64 $d0, $d5, $d6;          // $d0 = tx
    cvt_u64_u32 $d7, $s0;           // $d7 = work_item_id_y (ty)
    ld_kernarg_align(8)_width(all)_u64  $d8, [%__global_offset_1]; // $d8 = 0
    add_u64 $d2, $d7, $d8;          // $d2 = ty
    ld_kernarg_align(4)_width(all)_u32  $s0, [%wB];   // $s0 = wB
    ld_kernarg_align(4)_width(all)_u32  $s1, [%wA];   // $s1 = wA
    ld_kernarg_align(8)_width(all)_u64  $d1, [%C];    // $d1 = C
    cmp_lt_b1_s32   $c0, $s1, 1;                      // if (wA < 1)
    cbr_b1  $c0, @BB0_1;
    // BB#2:                                // %for.body.lr.ph
    ld_kernarg_align(8)_width(all)_u64  $d3, [%B];    // $d3 = B
    ld_kernarg_align(8)_width(all)_u64  $d4, [%A];    // $d4 = A
    add_u64 $d7, $d8, $d7;                        // $d7 = ty
    cvt_u32_u64 $s2, $d7;
    mul_u32 $s3, $s1, $s2;                       // $s3 = ty * wA
    add_u64 $d5, $d6, $d5;                       // $d5 = tx
    cvt_u32_u64 $s4, $d5;                        // $s4 = tx
    mov_b32 $s2, 0;                              // value = 0

@BB0_3:
    // %for.body
    cvt_s64_s32 $d5, $s4;        // $d5 = $s4
    shl_u64 $d5, $d5, 2;         // $d5 *= 4
    add_u64 $d5, $d3, $d5;       // $d5 = &B + (k*wB + tx)*4
    ld_global_align(4)_f32  $s5, [$d5]; // $s5 = B[k*wB + tx]
    cvt_s64_s32 $d5, $s3;        // $d5 = ty * wA + k
    shl_u64 $d5, $d5, 2;
    add_u64 $d5, $d4, $d5;       // $d5 = &A + (ty*wA+k)*4
    ld_global_align(4)_f32  $s6, [$d5]; // $s6 = A[ty*wA + k]
    mul_ftz_f32 $s5, $s6, $s5;          // $s5 = A[ty*wA + k] * B[k*wB + tx]
    add_ftz_f32 $s2, $s2, $s5;          // value += $s5
    add_u32 $s4, $s4, $s0;              // $s4 += wB, (++k)*wB
    add_u32 $s3, $s3, 1;                // $s3 += 1, ++k
    add_u32 $s1, $s1, 4294967295;      // $s1 += 0xFFFFFFFF
    cmp_ne_b1_s32   $c0, $s1, 0;       // $c0 = ($s1 != 0) , loop wA times
    cbr_b1  $c0, @BB0_3;
    br  @BB0_4;

@BB0_1:
    mov_b32 $s2, 0;

@BB0_4:
    // %for.end
    cvt_u32_u64 $s1, $d2;
    cvt_u32_u64 $s3, $d0;
    mad_u32 $s0, $s1, $s0, $s3;
    cvt_s64_s32 $d0, $s0;
    shl_u64 $d0, $d0, 2;
    add_u64 $d0, $d1, $d0;
    st_global_align(4)_f32  $s2, [$d0]; // C[ty*wB + tx] = value
    ret;
};
